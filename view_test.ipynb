{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "3\n",
      "{'axis': {'origin': [0, -0.007706040424053753, -0.24714714808389615], 'direction': [1, 0, 0]}, 'limit': {'a': 105.11999999999999, 'b': -27, 'noLimit': False}}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import sapien.core as sapien\n",
    "from PIL import Image\n",
    "import json\n",
    "from datagen.data_utils import *\n",
    "from pathlib import Path as P\n",
    "from utils.visualization import overlay_images\n",
    "# from utils.viewpoint import calculate_E2 as CE2\n",
    "# from utils.viewpoint import conversion_matrix_axis as cma\n",
    "from utils.viewpoint import * \n",
    "# from utils.viewpoint import conversion_matrix\n",
    "# %%\n",
    "# def setup_scene(urdf):\n",
    "#     # Set up the SAPIEN scene and camera\n",
    "#     camera, asset, scene = scene_setup(urdf_file=urdf)\n",
    "#     return camera, asset, scene\n",
    "# conversion_matrix = np.array([\n",
    "#     [0, 0, -1],\n",
    "#     [-1, 0, 0],\n",
    "#     [0, 1, 0]\n",
    "# # ])\n",
    "# def normalize(v):\n",
    "#     return v / np.sqrt(np.sum(v**2))\n",
    "\n",
    "# def calculate_E2(E1, axis_position, axis_direction, angle_degrees):\n",
    "#     # Convert the angle from degrees to radians\n",
    "#     angle_radians = degrees_to_radians(angle_degrees)\n",
    "    \n",
    "#     # Create a 3x3 rotation matrix around the axis\n",
    "#     R = get_rotation_axis_angle(axis_direction, angle_radians)\n",
    "#     # Create a 4x4 transformation matrix for the rotation\n",
    "#     rotation_matrix = np.eye(4)\n",
    "#     rotation_matrix[:3, :3] = R\n",
    "#     print(R)\n",
    "    \n",
    "#     # Create a 4x4 transformation matrix for translation to the axis position\n",
    "#     translation_matrix = np.eye(4)\n",
    "#     translation_matrix[:3, 3] = axis_position\n",
    "#     # Calculate E2 by combining translation and rotation with E1\n",
    "#     # E2 = np.dot(rotation_matrix, np.dot(rotation_matrix, E1))\n",
    "#     E2 = change_apply_change_basis(E1, rotation_matrix, translation_matrix)\n",
    "#     return E2\n",
    "\n",
    "# def change_apply_change_basis(A, T, P):\n",
    "#     \"\"\"\n",
    "#     Perform change of basis, apply transformation, and change back to the original basis.\n",
    "\n",
    "#     Args:\n",
    "#     A (numpy.ndarray): The original matrix in the B1 basis.\n",
    "#     T (numpy.ndarray): The transformation matrix to be applied.\n",
    "#     P (numpy.ndarray): The change of basis matrix from B1 to B2.\n",
    "\n",
    "#     Returns:\n",
    "#     numpy.ndarray: The resulting matrix after the entire process, expressed in the B1 basis.\n",
    "#     \"\"\"\n",
    "#     # Step 1: Change A to the B2 basis\n",
    "#     A_B2 = np.linalg.inv(P).dot(A).dot(P)\n",
    "    \n",
    "#     # Step 2: Apply the transformation T in the B2 basis\n",
    "#     A_B2_transformed = T.dot(A_B2)\n",
    "    \n",
    "#     # Step 3: Change A_B2_transformed back to the original B1 basis\n",
    "#     A_B1_transformed = P.dot(A_B2_transformed).dot(np.linalg.inv(P))\n",
    "    \n",
    "#     return A_B1_transformed\n",
    "\n",
    "# def get_rotation_axis_angle(k, theta):\n",
    "#     '''\n",
    "#     Rodrigues' rotation formula\n",
    "#     args:\n",
    "#     * k: direction vector of the axis to rotate about\n",
    "#     * theta: the (radian) angle to rotate with\n",
    "#     return:\n",
    "#     * 3x3 rotation matrix\n",
    "#     '''\n",
    "#     k = normalize(k)\n",
    "#     kx, ky, kz = k[0], k[1], k[2]\n",
    "#     cos, sin = np.cos(theta), np.sin(theta)\n",
    "#     R = np.zeros((3, 3))\n",
    "#     R[0, 0] = cos + (kx**2) * (1 - cos)\n",
    "#     R[0, 1] = kx * ky * (1 - cos) - kz * sin\n",
    "#     R[0, 2] = kx * kz * (1 - cos) + ky * sin\n",
    "#     R[1, 0] = kx * ky * (1 - cos) + kz * sin\n",
    "#     R[1, 1] = cos + (ky**2) * (1 - cos)\n",
    "#     R[1, 2] = ky * kz * (1 - cos) - kx * sin\n",
    "#     R[2, 0] = kx * kz * (1 - cos) - ky * sin\n",
    "#     R[2, 1] = ky * kz * (1 - cos) + kx * sin\n",
    "#     R[2, 2] = cos + (kz**2) * (1 - cos)\n",
    "#     return R\n",
    "\n",
    "\n",
    "# def transform_V1_to_V2(joint_state, V1, delta_rotation):\n",
    "#     try:\n",
    "#         # Extract the rotation matrix R_joint from the joint state\n",
    "#         R_joint = joint_state[:3, :3]\n",
    "\n",
    "#         u = R_joint[0, 2]\n",
    "#         v = R_joint[1, 2]\n",
    "#         w = R_joint[2, 2]\n",
    "\n",
    "\n",
    "#         # Calculate the rotation matrix R_theta for the delta rotation along the joint axis\n",
    "#         cos_theta = np.cos(delta_rotation)\n",
    "#         sin_theta = np.sin(delta_rotation)\n",
    "#         R_theta = np.array([[cos_theta + u**2 * (1 - cos_theta), u * v * (1 - cos_theta) - w * sin_theta, u * w * (1 - cos_theta) + v * sin_theta],\n",
    "#                             [v * u * (1 - cos_theta) + w * sin_theta, cos_theta + v**2 * (1 - cos_theta), v * w * (1 - cos_theta) - u * sin_theta],\n",
    "#                             [w * u * (1 - cos_theta) - v * sin_theta, w * v * (1 - cos_theta) + u * sin_theta, cos_theta + w**2 * (1 - cos_theta)]])\n",
    "\n",
    "\n",
    "#         # Create 4x4 transformation matrices\n",
    "#         T_joint = np.eye(4)\n",
    "#         T_joint[:3, :3] = R_joint\n",
    "\n",
    "#         T_theta = np.eye(4)\n",
    "#         T_theta[:3, :3] = R_theta\n",
    "\n",
    "#         # Compute V2 by multiplying the transformation matrices\n",
    "#         # V2 = np.dot(np.dot(np.dot(T_joint, T_theta), np.linalg.inv(T_joint)), V1)\n",
    "#         V2_prime = np.dot(np.linalg.inv(T_joint), np.dot(T_theta, V1))\n",
    "#         V2 = np.dot(V2_prime, T_joint)\n",
    "\n",
    "#         return V2\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error calculating V2: {str(e)}\")\n",
    "#         return None\n",
    "# def degrees_to_radians(degrees):\n",
    "#     radians = degrees * (math.pi / 180)\n",
    "#     return radians\n",
    "def render_image(camera, scene, save_path):\n",
    "    # Render an image using the provided camera and save it to the specified path\n",
    "    scene.step()\n",
    "    scene.update_render()\n",
    "    camera.take_picture()\n",
    "    rgba = camera.get_float_texture('Color')\n",
    "    rgba_img = (rgba * 255).clip(0, 255).astype(\"uint8\")\n",
    "    \n",
    "\n",
    "    seg_labels = camera.get_uint32_texture('Segmentation')  # [H, W, 4]\n",
    "    mask = seg_labels.sum(axis=-1)\n",
    "    mask[mask>0] = 1\n",
    "    rgba_img[:, :, -1] = rgba_img[:, :, -1] * mask\n",
    "    rgba_pil = Image.fromarray(rgba_img, 'RGBA')\n",
    "    rgba_pil.save(save_path)\n",
    "\n",
    "def load_json_to_dict(fname):\n",
    "    # Load JSON data from a file into a dictionary\n",
    "    try:\n",
    "        with open(fname, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON from {fname}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "urdf = 'data_base/laptop/10211/mobility.urdf'\n",
    "# Load JSON data\n",
    "json_fname = './data_base/laptop/10211/mobility_v2.json'\n",
    "meta = load_json_to_dict(json_fname)\n",
    "for link in meta:\n",
    "    if link['joint'] == 'hinge':\n",
    "        print(link['id'])\n",
    "        print(link['jointData'])\n",
    "        origin =  link['jointData']['axis']['origin']\n",
    "        direction =  link['jointData']['axis']['direction']\n",
    "# Setup the scene\n",
    "camera, asset, scene = scene_setup(urdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point\n",
      "(-3.503146346110184, -2.545184802275635, 2.5000000000000004)\n",
      "mat44:\n",
      "[[ 0.70062927 -0.58778525  0.4045085  -3.50314635]\n",
      " [ 0.50903696  0.80901699  0.29389263 -2.5451848 ]\n",
      " [-0.5         0.          0.8660254   2.5       ]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "[[ 0.89100652 -0.         -0.45399051]\n",
      " [ 0.          1.         -0.        ]\n",
      " [ 0.45399051  0.          0.89100652]]\n",
      "Pose([0, 0, 0], [0.97237, 0, -0.233445, 0])\n",
      "P\n",
      " [[ 1.          0.          0.          0.24714715]\n",
      " [ 0.          1.          0.          0.        ]\n",
      " [ 0.          0.          1.         -0.00770604]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "t\n",
      " [[ 0.70062927 -0.58778525  0.4045085  -3.75029349]\n",
      " [ 0.50903696  0.80901699  0.29389263 -2.5451848 ]\n",
      " [-0.5         0.          0.8660254   2.50770604]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "A_B2\n",
      " [[ 0.70062927 -0.58778525  0.4045085  -3.58025213]\n",
      " [ 0.50903696  0.80901699  0.29389263 -2.42164252]\n",
      " [-0.5         0.          0.8660254   2.37745884]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "A_B2_transformed\n",
      " [[ 0.8512605  -0.52372049 -0.03274761 -4.26937174]\n",
      " [ 0.50903696  0.80901699  0.29389263 -2.42164252]\n",
      " [-0.12742422 -0.26684893  0.9552773   0.49293083]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# point = random_point_in_sphere(5, theta_range=[-0.01 * math.pi, 0.01*math.pi], phi_range=[0.01*math.pi, 0.01*math.pi])\n",
    "\n",
    "def point_in_sphere(r, theta, phi):\n",
    "    x = r * math.sin(phi) * math.cos(theta)\n",
    "    y = r * math.sin(phi) * math.sin(theta)\n",
    "    z = r * math.cos(phi)\n",
    "    \n",
    "    return x, y, z\n",
    "\n",
    "point = point_in_sphere(5, 1.2*math.pi, degrees_to_radians(60))\n",
    "mat44 = calculate_cam_ext(point)\n",
    "print(\"point\")\n",
    "print(point)\n",
    "print('mat44:')\n",
    "print(mat44)\n",
    "# print(point)\n",
    "# mat44 = np.eye(4)\n",
    "# mat44[0, -1] = 4\n",
    "# mat44[1, -1] = 2\n",
    "# mat44[2, -1] = 1\n",
    "camera.set_pose(sapien.Pose.from_transformation_matrix(mat44))\n",
    "# Render image with default pose\n",
    "render_image(camera, scene, './draft/test_view.png')\n",
    "seg_labels = camera.get_uint32_texture('Segmentation')  # [H, W, 4]\n",
    "seg_view = seg_labels[..., 1].astype(np.uint8)  # actor-level\n",
    "seg_view[seg_view != 2] = 0\n",
    "seg_pil = Image.fromarray(seg_view*255)\n",
    "seg_pil.save('./draft/test_view_seg.png')\n",
    "view_0 = camera.get_model_matrix()\n",
    "art_degree = radians_to_degree(asset.get_qlimits()[0, 1])\n",
    "\n",
    "# Rotate the joint and render a new image\n",
    "asset.set_qpos(degrees_to_radians(art_degree))\n",
    "render_image(camera, scene, './draft/test_view_15.png')\n",
    "seg_labels = camera.get_uint32_texture('Segmentation')  # [H, W, 4]\n",
    "seg_view_15 = seg_labels[..., 1].astype(np.uint8)  # actor-level\n",
    "seg_view_15[seg_view_15 != 1] = 0\n",
    "seg_view_15 = Image.fromarray(seg_view_15*255)\n",
    "seg_view_15.save('./draft/test_view_15_seg.png')\n",
    "# Calculate joint state and transformation\n",
    "joint_state = asset.get_links()[-1].pose.to_transformation_matrix()\n",
    "# v2 = transform_V1_to_V2(joint_state, mat44, degrees_to_radians(15))\n",
    "\n",
    "\n",
    "# dirs, ori = np.dot(conversion_matrix, direction), np.dot(conversion_matrix, origin)\n",
    "\n",
    "\n",
    "# v2 = calculate_E2(mat44, ori, dirs, art_degree)\n",
    "v2 = calculate_E2(mat44, origin, direction, art_degree)\n",
    "# view_2 = calculate_E2(view_0, np.array(origin), np.array(direction), art_degree)\n",
    "# Set the new joint configuration and render the image\n",
    "asset.set_qpos(0)\n",
    "camera.set_pose(sapien.Pose.from_transformation_matrix(v2))\n",
    "render_image(camera, scene, './draft/test_view_v2.png')\n",
    "seg_labels = camera.get_uint32_texture('Segmentation')  # [H, W, 4]\n",
    "seg_view_15_v2 = seg_labels[..., 1].astype(np.uint8)  # actor-level\n",
    "seg_view_15_v2[seg_view_15_v2 != 3] = 0\n",
    "seg_view_15_v2_pil = Image.fromarray(seg_view_15_v2*255)\n",
    "seg_view_15_v2_pil.save('./draft/test_view_v2_seg.png')\n",
    "\n",
    "view_1 = camera.get_model_matrix()\n",
    "\n",
    "\n",
    "inerf_json = {\n",
    "    'view_0': view_0.tolist(),\n",
    "    'view_1': view_1.tolist()\n",
    "}\n",
    "\n",
    "with open('./draft/inerf_pose.json', 'w') as f:\n",
    "    json.dump(inerf_json, f)\n",
    "\n",
    "\n",
    "# Overlay images\n",
    "rgba_15_pil = Image.open('./draft/test_view_15.png')\n",
    "rgba_v2_pil = Image.open('./draft/test_view_v2.png')\n",
    "overlayed = overlay_images(rgba_15_pil, rgba_v2_pil, 0.5)\n",
    "overlayed.save('./draft/overlayed_view.png')\n",
    "\n",
    "\n",
    "rgba_pil = Image.open('./draft/test_view.png')\n",
    "overlayed = overlay_images(rgba_pil, rgba_15_pil, 0.5)\n",
    "overlayed.save('./draft/overlayed_art.png')\n",
    "\n",
    "overlayed = overlay_images(rgba_pil, rgba_v2_pil, 0.5)\n",
    "overlayed.save('./draft/overlayed_base.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  0.2471],\n",
      "        [ 0.0000,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.0000, -0.0077],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "t\n",
      " tensor([[ 7.0063e-01, -5.8779e-01,  4.0451e-01, -3.7503e+00],\n",
      "        [ 5.0904e-01,  8.0902e-01,  2.9389e-01, -2.5452e+00],\n",
      "        [-5.0000e-01,  2.9802e-08,  8.6603e-01,  2.5077e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]])\n",
      "A_B2\n",
      " tensor([[ 7.0063e-01, -5.8779e-01,  4.0451e-01, -3.5803e+00],\n",
      "        [ 5.0904e-01,  8.0902e-01,  2.9389e-01, -2.4216e+00],\n",
      "        [-5.0000e-01,  2.9802e-08,  8.6603e-01,  2.3775e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]])\n",
      "A_B2_transformed\n",
      " tensor([[ 0.8513, -0.5237, -0.0327, -4.2694],\n",
      "        [ 0.5090,  0.8090,  0.2939, -2.4216],\n",
      "        [-0.1274, -0.2668,  0.9553,  0.4929],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "tensor([[ 0.5237, -0.0327, -0.8513, -4.2329],\n",
      "        [-0.8090,  0.2939, -0.5090, -2.5452],\n",
      "        [ 0.2668,  0.9553,  0.1274,  0.5241],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "tensor([[ 0.5237, -0.0327, -0.8513, -4.2329],\n",
      "        [-0.8090,  0.2939, -0.5090, -2.5452],\n",
      "        [ 0.2668,  0.9553,  0.1274,  0.5241],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "tensor([[ 2.9802e-07,  6.2957e-07,  1.7881e-07,  0.0000e+00],\n",
      "        [ 1.1921e-07,  5.9605e-08, -5.9605e-08,  0.0000e+00],\n",
      "        [-3.5763e-07, -5.9605e-08,  7.1526e-07,  3.1590e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# verify view0, view1\n",
    "from utils.rotation import R_from_quaternions\n",
    "view_0_torch = torch.Tensor(view_0)\n",
    "view_1_torch = torch.Tensor(view_1)\n",
    "origin_torch = torch.Tensor(origin)\n",
    "Q = torch.Tensor([0.97237, 0, -0.233445, 0])\n",
    "c2w = view_0_torch\n",
    "E1 = view2pose_torch(c2w)\n",
    "translation_matrix = torch.eye(4).to(c2w)\n",
    "translation_matrix[:3, 3] = convert_ori_torch(origin_torch).view([3])\n",
    "rotation_matrix = torch.eye(4).to(c2w)\n",
    "R = R_from_quaternions(Q)\n",
    "rotation_matrix[:3, :3] = R\n",
    "E2 = change_apply_change_basis_torch(E1, rotation_matrix, translation_matrix)\n",
    "view_1_test = pose2view_torch(E2)\n",
    "print(view_1_test)\n",
    "print(view_1_torch)\n",
    "print(view_1_test - view_1_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70062927 -0.58778525  0.4045085  -3.50314635]\n",
      " [ 0.50903696  0.80901699  0.29389263 -2.5451848 ]\n",
      " [-0.5         0.          0.8660254   2.5       ]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(mat44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8512605  -0.52372049 -0.03274761 -4.23286355]\n",
      " [ 0.50903696  0.80901699  0.29389263 -2.5451848 ]\n",
      " [-0.12742422 -0.26684893  0.9552773   0.52407873]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pose_img = 10\n",
    "dof = asset.dof\n",
    "qlimits = asset.get_qlimits()\n",
    "qrange = qlimits[:, 1] - qlimits[:, 0]\n",
    "qpos_list = np.random.randn(num_pose_img, dof)\n",
    "qpos_list = qpos_list * qrange - qlimits[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dof = asset.dof\n",
    "asset.set_qpos([np.inf] * dof)\n",
    "qpos = asset.get_qpos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4712389], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.89100653 -0.         -0.45399052]\n",
      " [ 0.          1.         -0.        ]\n",
      " [ 0.45399052  0.          0.89100653]]\n"
     ]
    }
   ],
   "source": [
    "ori, dirs, pos = get_local_art_GT(origin, direction, qpos, is_degree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.24714715  0.         -0.00770604]\n",
      "[ 0 -1  0]\n",
      "Pose([0, 0, 0], [0.97237, 0, -0.233445, 0])\n"
     ]
    }
   ],
   "source": [
    "print(ori)\n",
    "print(dirs)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform test with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.10904854e-07  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.11001687e-07  0.00000000e+00 -1.16971783e-07  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "diff = view_0 - pose2view(mat44)\n",
    "diff[abs(diff) < 1e-7] = 0\n",
    "print(diff)\n",
    "\n",
    "diff = mat44 - view2pose(pose2view(mat44))\n",
    "diff[abs(diff) < 1e-7] = 0\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform test with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.91865003, -0.07997948,  0.24615154,  0.2984875 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "mat44_torch = torch.Tensor(mat44)\n",
    "view_torch = torch.Tensor(view_0)\n",
    "pose = sapien.Pose.from_transformation_matrix(mat44)\n",
    "pose.q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 1.1921e-07,  0.0000e+00, -1.1921e-07,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "diff = view_torch - pose2view_torch(mat44_torch)\n",
    "diff[abs(diff) < 1e-7] = 0\n",
    "print(diff)\n",
    "\n",
    "diff = mat44_torch - view2pose_torch(pose2view_torch(mat44_torch))\n",
    "diff[abs(diff) < 1e-7] = 0\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori, dirs = convert_ori_dir(origin, direction)\n",
    "\n",
    "# Convert the angle from degrees to radians\n",
    "angle_radians = degrees_to_radians(art_degree)\n",
    "\n",
    "# Create a 3x3 rotation matrix around the axis\n",
    "R = get_rotation_axis_angle(dirs, angle_radians)\n",
    "\n",
    "from utils.rotation import *\n",
    "Q = matrix_to_quaternion(torch.Tensor(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9724,  0.0000, -0.2334,  0.0000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9723699 ,  0.        , -0.23344538,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mat44 = np.eye(4)\n",
    "test_mat44[:3, :3] = R\n",
    "test_pose = sapien.Pose.from_transformation_matrix(test_mat44)\n",
    "test_pose.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24714715,  0.        , -0.00770604])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "joints = asset.get_joints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sapien.core.pysapien.KinematicJointFixed object at 0x7efc2cceaf70>\n",
      "<sapien.core.pysapien.KinematicJointFixed object at 0x7efc2ccea7f0>\n",
      "<sapien.core.pysapien.KinematicJointRevolute object at 0x7efc2cceae30>\n"
     ]
    }
   ],
   "source": [
    "for joint in joints:\n",
    "    print(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pose([0, -0.00770604, -0.247147], [-0, 0, 1, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint.get_pose_in_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset.set_qpos(np.array([0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset.get_qpos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pose([0, 0, 0], [-0, 0, 1, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset.get_joints()[-1].get_pose_in_child()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = joint.get_child_link()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pose([0.247147, 0, -0.00770605], [0.5, 0.5, -0.5, -0.5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
