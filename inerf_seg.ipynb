{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iNeRF with segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/torchmetrics/utilities/imports.py:24: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _PYTHON_LOWER_3_8 = LooseVersion(_PYTHON_VERSION) < LooseVersion(\"3.8\")\n",
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/torchmetrics/utilities/imports.py:24: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _PYTHON_LOWER_3_8 = LooseVersion(_PYTHON_VERSION) < LooseVersion(\"3.8\")\n",
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n",
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/pytorch_lightning/__init__.py:28: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.vanilla_nerf.model_nerfseg import NeRFSeg\n",
    "from models.vanilla_nerf.helper import load_state_dict_and_report\n",
    "from PIL import Image\n",
    "from pathlib import  Path as P\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from datasets.ray_utils import get_ray_directions\n",
    "import matplotlib.pyplot as plt\n",
    "from models.vanilla_nerf.model_nerfseg import  get_rays_torch\n",
    "from models.vanilla_nerf.helper import img2mse\n",
    "# from pytorch3d.transforms import quaternion_to_matrix\n",
    "import torch.nn.functional as F\n",
    "from utils.viewpoint import pose2view_torch, view2pose_torch, change_apply_change_basis_torch\n",
    "from models.vanilla_nerf.helper import img2mse_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def load_dict_and_report(model, pretrained_state_dict):\n",
    "    # Get the model's state_dict\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    # Initialize lists to store missing and unexpected keys\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "\n",
    "    # Iterate through the keys in the pretrained_state_dict\n",
    "    for key, value in pretrained_state_dict.items():\n",
    "        if key in model_state_dict:\n",
    "            if model_state_dict[key].shape == value.shape:\n",
    "                model_state_dict[key] = value\n",
    "            else:\n",
    "                print(f\"Size mismatch for key '{key}': expected {model_state_dict[key].shape}, but got {value.shape}\")\n",
    "        else:\n",
    "            missing_keys.append(key)\n",
    "\n",
    "    # Check for unexpected keys\n",
    "    for key in model_state_dict.keys():\n",
    "        if key not in pretrained_state_dict:\n",
    "            unexpected_keys.append(key)\n",
    "\n",
    "    # Load the modified state_dict into the model\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    # Report missing and unexpected keys\n",
    "    if missing_keys:\n",
    "        print(\"Missing keys in model state_dict:\")\n",
    "        for key in missing_keys:\n",
    "            print(key)\n",
    "\n",
    "    if unexpected_keys:\n",
    "        print(\"Unexpected keys in pretrained state_dict:\")\n",
    "        for key in unexpected_keys:\n",
    "            print(key)\n",
    "\n",
    "            \n",
    "from utils.rotation import *\n",
    "from utils.viewpoint import *\n",
    "class ArticulationEstimation(nn.Module):\n",
    "    '''\n",
    "    Current implemetation for revolute only\n",
    "    '''\n",
    "    def __init__(self, mode='qua') -> None:\n",
    "        super().__init__()\n",
    "        if mode == 'qua':\n",
    "            pass\n",
    "        elif mode == 'rad': #radian\n",
    "            pass\n",
    "        elif mode == 'deg': # degree\n",
    "            pass\n",
    "        else:\n",
    "            raise RuntimeError('mode == %s for ArticulationEstimation is not defined' % mode)\n",
    "        \n",
    "        # perfect init\n",
    "        # init_Q = torch.Tensor([ 0.9962,  0.0000, -0.0872,  0.0000])\n",
    "        # axis_origin = torch.Tensor([ 0.24714715,  0.        , -0.00770604])\n",
    "        # normal init\n",
    "        init_Q = torch.Tensor([1, 0, 0, 0])\n",
    "        axis_origin = torch.Tensor([ 0, 0, 0])\n",
    "\n",
    "        # axis angle can be obtained from quaternion\n",
    "        # axis_direction = torch.Tensor([0, 0, 0])\n",
    "\n",
    "        self.Q = nn.Parameter(init_Q, requires_grad = True)\n",
    "        self.axis_origin = nn.Parameter(axis_origin, requires_grad = True)\n",
    "        # self.axis_direction = nn.Parameter(axis_direction, requires_grad = True)\n",
    "\n",
    "\n",
    "    def forward(self, c2w) -> torch.Tensor():\n",
    "        '''\n",
    "        input: c2w\n",
    "        '''\n",
    "        E1 = view2pose_torch(c2w)\n",
    "        translation_matrix = torch.eye(4).to(c2w)\n",
    "        translation_matrix[:3, 3] = self.axis_origin.view([3])\n",
    "        rotation_matrix = torch.eye(4).to(c2w)\n",
    "        R = R_from_quaternions(self.Q)\n",
    "        rotation_matrix[:3, :3] = R\n",
    "        E2 = change_apply_change_basis_torch(E1, rotation_matrix, translation_matrix)\n",
    "        view = pose2view_torch(E2)\n",
    "        return view\n",
    "def fetch_img(root_path, transform_meta, w=640, h=480, device='cuda', if_fix=True):\n",
    "    if if_fix:\n",
    "        idx = 5\n",
    "    else:\n",
    "        idx = np.random.randint(0, 9)\n",
    "    frame_id = 'r_' + str(idx)\n",
    "    pose_np = np.array(transform_meta['frame'][frame_id])\n",
    "\n",
    "    rgb_pil = Image.open(str(root_path/'rgb'/(frame_id + '.png')))\n",
    "    rgb = transforms.ToTensor()(rgb_pil).to(device)\n",
    "    rgb = rgb.view(4, -1).permute(1, 0) # (H*W, 4) RGBA\n",
    "    rgb = rgb[:, :3]*rgb[:, -1:] + (1-rgb[:, -1:]) # blend A to RGB\n",
    "\n",
    "    pose = torch.Tensor(pose_np).to(device)\n",
    "\n",
    "    seg_pil = Image.open(str(root_path/'seg'/(frame_id + '.png')))\n",
    "    seg_np = np.array(seg_pil)\n",
    "    seg = torch.Tensor(seg_np).to(device).view([1, -1]).permute(1, 0)\n",
    "    seg = seg.type(torch.LongTensor)\n",
    "    seg = seg - 1 # starts with 2\n",
    "    seg[seg<0] = 0\n",
    "    focal = transform_meta['focal']\n",
    "    directions = get_ray_directions(h, w, focal).view([-1, 3])\n",
    "    mask = seg > 0\n",
    "    mask = mask.to(rgb)\n",
    "    ret_dict = {\n",
    "        'rgb': rgb.to(device),\n",
    "        'seg': seg.to(device),\n",
    "        'directions': directions.to(device),\n",
    "        'pose': pose.to(device),\n",
    "        'mask': mask.to(device)\n",
    "    }\n",
    "    return ret_dict\n",
    "\n",
    "def load_json(json_fname):\n",
    "    with open(json_fname, 'r') as json_file:\n",
    "        data_dict = json.load(json_file)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NeRFSeg models with NeRF checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected keys in pretrained state_dict:\n",
      "coarse_mlp.seg_layer.weight\n",
      "coarse_mlp.seg_layer.bias\n",
      "fine_mlp.seg_layer.weight\n",
      "fine_mlp.seg_layer.bias\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from opt import get_opts\n",
    "\n",
    "sys.argv = ['', '--config', 'config/train_nerfseg.json']\n",
    "device = 'cuda'\n",
    "hparams= get_opts()\n",
    "nerfseg = NeRFSeg(hparams)\n",
    "ckpt = torch.load(hparams.nerf_ckpt)\n",
    "load_dict = {}\n",
    "length = len('model.')\n",
    "state_dict = ckpt['state_dict']\n",
    "for key in state_dict.keys():\n",
    "    if key[:length] == 'model.':\n",
    "        load_dict[key[length:]] = state_dict[key]\n",
    "\n",
    "load_dict_and_report(nerfseg, load_dict)\n",
    "# load_state_dict_and_report(nerfseg, hparams.nerf_ckpt)\n",
    "nerfseg = nerfseg.to(device)\n",
    "art_est = ArticulationEstimation().to(device)\n",
    "near = 2\n",
    "far = 6\n",
    "art_dict = {}\n",
    "for part in range(hparams.part_num):\n",
    "    if part == 0:\n",
    "        part_art_est = None\n",
    "    else:\n",
    "        part_art_est = ArticulationEstimation().to(device)\n",
    "    art_dict[str(part)] = part_art_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_params = []\n",
    "for name, param in nerfseg.named_parameters():\n",
    "    if 'seg' in name:\n",
    "        seg_params += [param]\n",
    "\n",
    "art_params = []\n",
    "for _, art_est in art_dict.items():\n",
    "    if art_est is not None:\n",
    "        for _, param in art_est.named_parameters():\n",
    "            param.requires_grad = True\n",
    "            art_params += [param]\n",
    "\n",
    "art_lr = 1e-2\n",
    "seg_lr = 1e-3\n",
    "\n",
    "seg_opt_dict = {\n",
    "    'params': seg_params,\n",
    "    'lr': seg_lr\n",
    "}\n",
    "\n",
    "art_opt_dict = {\n",
    "    'params': art_params,\n",
    "    'lr': art_lr\n",
    "}\n",
    "\n",
    "# optimizer = torch.optim.Adam([seg_opt_dict, art_opt_dict], lr=seg_lr)\n",
    "\n",
    "optimizer = torch.optim.Adam([seg_opt_dict], lr=seg_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# result = []\n",
    "data_root = P(\"./data/laptop_art_same_pose/train/idx_5/\")\n",
    "transform_meta = load_json(str(data_root / 'transforms.json'))\n",
    "ray_chunk_size = 4096\n",
    "data_dict = fetch_img(data_root, transform_meta)\n",
    "\n",
    "# directions = data_dict['directions']\n",
    "# rgb = data_dict['rgb']\n",
    "# seg = data_dict['seg']\n",
    "# pose = data_dict['pose']\n",
    "# mask = data_dict['mask']\n",
    "# random_indx = torch.randint(0, directions.shape[0], [ray_chunk_size])\n",
    "# random_dirs = directions[random_indx]\n",
    "# random_rgbs = rgb[random_indx]\n",
    "# random_mask = mask[random_indx]\n",
    "# target_dict = {\n",
    "#     'rgb': random_rgbs,\n",
    "#     'mask': random_mask\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_dict = {}\n",
    "# for part_id, part_art in art_dict.items():\n",
    "#     if part_art is None:\n",
    "#         new_pose = pose\n",
    "#     else:\n",
    "#         new_pose = part_art(pose)\n",
    "#     rays_o, viewdirs, rays_d = get_rays_torch(random_dirs, new_pose[:3, :], output_view_dirs=True)\n",
    "#     # gather input_dict for NeRF\n",
    "#     input_dict = {\n",
    "#         'rays_o': rays_o,\n",
    "#         'rays_d': rays_d,\n",
    "#         'viewdirs': viewdirs\n",
    "#     }\n",
    "#     part_code = torch.zeros([1, hparams.part_num]).to(rays_o)\n",
    "#     part_code[:, int(part_id)] = 1\n",
    "#     input_dict['part_code'] = part_code\n",
    "#     rendered_results = nerfseg(input_dict, False, True, near, far)\n",
    "#     result_dict[part_id] = rendered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo_loss(result_dict: dict(), target_dict: dict()):\n",
    "    ret_dict = {}\n",
    "    for part_id, result in result_dict.items():\n",
    "        coarse_dict = result['level_0']\n",
    "        fine_dict = result['level_1']\n",
    "        rgb_c = coarse_dict['rgb']\n",
    "        rgb_f = fine_dict['rgb']\n",
    "        seg_c = coarse_dict['comp_seg']\n",
    "        seg_f = fine_dict['comp_seg']\n",
    "\n",
    "        rgb_target = target_dict['rgb']\n",
    "\n",
    "        photo_loss_c = img2mse_weighted(rgb_c, rgb_target, seg_c)\n",
    "        photo_loss_f = img2mse_weighted(rgb_f, rgb_target, seg_f)\n",
    "        \n",
    "        ret_part_dict = {\n",
    "            'photo_c': photo_loss_c,\n",
    "            'photo_f': photo_loss_f\n",
    "        }\n",
    "        ret_dict[part_id] = ret_part_dict\n",
    "    \n",
    "    return ret_dict\n",
    "\n",
    "def get_mask_loss(result_dict: dict(), target_dict: dict()):\n",
    "    acc_f_result = []\n",
    "    acc_c_result = []\n",
    "    for part_id, result in result_dict.items():\n",
    "        coarse_dict = result['level_0']\n",
    "        fine_dict = result['level_1']\n",
    "        seg_c = coarse_dict['comp_seg']\n",
    "        seg_f = fine_dict['comp_seg']\n",
    "        acc_c = coarse_dict['acc'].view([-1, 1])\n",
    "        acc_f = fine_dict['acc'].view([-1, 1])\n",
    "        acc_c_result += [seg_c * acc_c]\n",
    "        acc_f_result += [seg_f * acc_f]\n",
    "\n",
    "    all_acc_c = torch.cat(acc_c_result, dim=-1)\n",
    "    all_acc_f = torch.cat(acc_f_result, dim=-1)\n",
    "\n",
    "    max_acc_c, _ = all_acc_c.max(dim=-1, keepdim=True)\n",
    "    max_acc_f, _ = all_acc_f.max(dim=-1, keepdim=True)\n",
    "    acc_target = target_dict['mask'].view([-1, 1])\n",
    "    loss_mask_c = torch.nn.BCELoss()(max_acc_c, acc_target)\n",
    "    loss_mask_f = torch.nn.BCELoss()(max_acc_f, acc_target)\n",
    "    ret_dict = {\n",
    "        'loss_mask_c': loss_mask_c,\n",
    "        'loss_mask_f': loss_mask_f\n",
    "    }\n",
    "    return ret_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo_loss_total(photo_dict):\n",
    "    loss_list = []\n",
    "    for part_id, part_dict in photo_dict.items():\n",
    "        for _, loss in part_dict.items():\n",
    "            loss_list += [loss]\n",
    "\n",
    "    total_loss = sum(loss_list)\n",
    "    return total_loss\n",
    "\n",
    "def get_mask_total_loss(mask_dict):\n",
    "    loss_list = []\n",
    "    for _, v in mask_dict.items():\n",
    "        loss_list += [v]\n",
    "    return sum(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at step 0: mask loss: 0.30853, photo loss: 0.01246, total loss: 0.32099\n",
      "Results at step 10: mask loss: 0.28327, photo loss: 0.01045, total loss: 0.29372\n",
      "Results at step 20: mask loss: 0.31596, photo loss: 0.00839, total loss: 0.32435\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dj/git/articulated-object-nerf/inerf_seg.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/dj/git/articulated-object-nerf/inerf_seg.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     part_code[:, \u001b[39mint\u001b[39m(part_id)] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/dj/git/articulated-object-nerf/inerf_seg.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     input_dict[\u001b[39m'\u001b[39m\u001b[39mpart_code\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m part_code\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/dj/git/articulated-object-nerf/inerf_seg.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     rendered_results \u001b[39m=\u001b[39m nerfseg(input_dict, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, near, far)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/dj/git/articulated-object-nerf/inerf_seg.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     result_dict[part_id] \u001b[39m=\u001b[39m rendered_results\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/dj/git/articulated-object-nerf/inerf_seg.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m photo_dict \u001b[39m=\u001b[39m get_photo_loss(result_dict, target_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/ao/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/articulated-object-nerf/models/vanilla_nerf/model_nerfseg.py:314\u001b[0m, in \u001b[0;36mNeRFSeg.forward\u001b[0;34m(self, rays, randomized, white_bkgd, near, far, seg_feat)\u001b[0m\n\u001b[1;32m    312\u001b[0m density \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma_activation(raw_density)\n\u001b[1;32m    313\u001b[0m seg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseg_activation(raw_seg)\n\u001b[0;32m--> 314\u001b[0m render_dict \u001b[39m=\u001b[39m helper\u001b[39m.\u001b[39;49mvolumetric_rendering_with_seg(\n\u001b[1;32m    315\u001b[0m     rgb, \n\u001b[1;32m    316\u001b[0m     density,\n\u001b[1;32m    317\u001b[0m     t_vals,\n\u001b[1;32m    318\u001b[0m     rays[\u001b[39m\"\u001b[39;49m\u001b[39mrays_d\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    319\u001b[0m     white_bkgd\u001b[39m=\u001b[39;49mwhite_bkgd,\n\u001b[1;32m    320\u001b[0m     seg\u001b[39m=\u001b[39;49mseg,\n\u001b[1;32m    321\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhparams\u001b[39m.\u001b[39;49mseg_mode\n\u001b[1;32m    322\u001b[0m )\n\u001b[1;32m    324\u001b[0m \u001b[39m# save for sample_pdf function for fine mlp\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39m# weights = render_dict['weights']\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \n\u001b[1;32m    327\u001b[0m \u001b[39m# ret.append((comp_rgb, acc, depth, seg_result))\u001b[39;00m\n\u001b[1;32m    328\u001b[0m feat_out \u001b[39m=\u001b[39m mlp_ret_dict\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/git/articulated-object-nerf/models/vanilla_nerf/helper.py:297\u001b[0m, in \u001b[0;36mvolumetric_rendering_with_seg\u001b[0;34m(rgb, density, t_vals, dirs, white_bkgd, seg, nocs, mode)\u001b[0m\n\u001b[1;32m    294\u001b[0m     final_seg_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((seg_bg_weights, seg_fg_weights), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    295\u001b[0m     comp_seg \u001b[39m=\u001b[39m (final_seg_weights \u001b[39m*\u001b[39m seg)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 297\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(comp_rgb)\u001b[39m.\u001b[39many():\n\u001b[1;32m    298\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnan in rgb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    300\u001b[0m depth \u001b[39m=\u001b[39m (weights \u001b[39m*\u001b[39m t_vals)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "optimize_step = 200\n",
    "for training_step in range(optimize_step):\n",
    "    optimizer.zero_grad()\n",
    "    data_dict = fetch_img(data_root, transform_meta)\n",
    "\n",
    "    directions = data_dict['directions']\n",
    "    rgb = data_dict['rgb']\n",
    "    seg = data_dict['seg']\n",
    "    pose = data_dict['pose']\n",
    "    mask = data_dict['mask']\n",
    "    random_indx = torch.randint(0, directions.shape[0], [ray_chunk_size])\n",
    "    random_dirs = directions[random_indx]\n",
    "    random_rgbs = rgb[random_indx]\n",
    "    random_mask = mask[random_indx]\n",
    "    target_dict = {\n",
    "        'rgb': random_rgbs,\n",
    "        'mask': random_mask\n",
    "    }\n",
    "    result_dict = {}\n",
    "    for part_id, part_art in art_dict.items():\n",
    "        if part_art is None:\n",
    "            new_pose = pose\n",
    "        else:\n",
    "            new_pose = part_art(pose)\n",
    "        rays_o, viewdirs, rays_d = get_rays_torch(random_dirs, new_pose[:3, :], output_view_dirs=True)\n",
    "        # gather input_dict for NeRF\n",
    "        input_dict = {\n",
    "            'rays_o': rays_o,\n",
    "            'rays_d': rays_d,\n",
    "            'viewdirs': viewdirs\n",
    "        }\n",
    "        part_code = torch.zeros([1, hparams.part_num]).to(rays_o)\n",
    "        part_code[:, int(part_id)] = 1\n",
    "        input_dict['part_code'] = part_code\n",
    "        rendered_results = nerfseg(input_dict, False, True, near, far)\n",
    "        result_dict[part_id] = rendered_results\n",
    "    photo_dict = get_photo_loss(result_dict, target_dict)\n",
    "    mask_dict = get_mask_loss(result_dict, target_dict)\n",
    "    photo_loss = get_photo_loss_total(photo_dict) \n",
    "    mask_loss = get_mask_total_loss(mask_dict)\n",
    "    total_loss = photo_loss + mask_loss\n",
    "    if training_step % 10 == 0:\n",
    "        print('Results at step %d: mask loss: %.5f, photo loss: %.5f, total loss: %.5f' %(training_step, mask_loss.item(), photo_loss.item(), total_loss.item()))\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at step 999: mask loss: 0.17218, photo loss: 0.01001, total loss: 0.18219\n"
     ]
    }
   ],
   "source": [
    "print('Results at step %d: mask loss: %.5f, photo loss: %.5f, total loss: %.5f' %(training_step, mask_loss.item(), photo_loss.item(), total_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2190,  0.0573, -0.1077], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_dict['1'].axis_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
