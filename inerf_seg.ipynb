{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iNeRF with segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/torchmetrics/utilities/imports.py:24: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _PYTHON_LOWER_3_8 = LooseVersion(_PYTHON_VERSION) < LooseVersion(\"3.8\")\n",
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/torchmetrics/utilities/imports.py:24: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _PYTHON_LOWER_3_8 = LooseVersion(_PYTHON_VERSION) < LooseVersion(\"3.8\")\n",
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n",
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/pytorch_lightning/__init__.py:28: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.vanilla_nerf.model_nerfseg import NeRFSeg\n",
    "from models.vanilla_nerf.helper import load_state_dict_and_report\n",
    "from PIL import Image\n",
    "from pathlib import  Path as P\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from datasets.ray_utils import get_ray_directions\n",
    "import matplotlib.pyplot as plt\n",
    "from models.vanilla_nerf.model_nerfseg import  get_rays_torch\n",
    "from models.vanilla_nerf.helper import img2mse\n",
    "# from pytorch3d.transforms import quaternion_to_matrix\n",
    "import torch.nn.functional as F\n",
    "from utils.viewpoint import pose2view_torch, view2pose_torch, change_apply_change_basis_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def load_dict_and_report(model, pretrained_state_dict):\n",
    "    # Get the model's state_dict\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    # Initialize lists to store missing and unexpected keys\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "\n",
    "    # Iterate through the keys in the pretrained_state_dict\n",
    "    for key, value in pretrained_state_dict.items():\n",
    "        if key in model_state_dict:\n",
    "            if model_state_dict[key].shape == value.shape:\n",
    "                model_state_dict[key] = value\n",
    "            else:\n",
    "                print(f\"Size mismatch for key '{key}': expected {model_state_dict[key].shape}, but got {value.shape}\")\n",
    "        else:\n",
    "            missing_keys.append(key)\n",
    "\n",
    "    # Check for unexpected keys\n",
    "    for key in model_state_dict.keys():\n",
    "        if key not in pretrained_state_dict:\n",
    "            unexpected_keys.append(key)\n",
    "\n",
    "    # Load the modified state_dict into the model\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    # Report missing and unexpected keys\n",
    "    if missing_keys:\n",
    "        print(\"Missing keys in model state_dict:\")\n",
    "        for key in missing_keys:\n",
    "            print(key)\n",
    "\n",
    "    if unexpected_keys:\n",
    "        print(\"Unexpected keys in pretrained state_dict:\")\n",
    "        for key in unexpected_keys:\n",
    "            print(key)\n",
    "\n",
    "            \n",
    "from utils.rotation import *\n",
    "from utils.viewpoint import *\n",
    "class ArticulationEstimation(nn.Module):\n",
    "    '''\n",
    "    Current implemetation for revolute only\n",
    "    '''\n",
    "    def __init__(self, mode='qua') -> None:\n",
    "        super().__init__()\n",
    "        if mode == 'qua':\n",
    "            pass\n",
    "        elif mode == 'rad': #radian\n",
    "            pass\n",
    "        elif mode == 'deg': # degree\n",
    "            pass\n",
    "        else:\n",
    "            raise RuntimeError('mode == %s for ArticulationEstimation is not defined' % mode)\n",
    "        \n",
    "        # perfect init\n",
    "        # init_Q = torch.Tensor([ 0.9962,  0.0000, -0.0872,  0.0000])\n",
    "        # axis_origin = torch.Tensor([ 0.24714715,  0.        , -0.00770604])\n",
    "        # normal init\n",
    "        init_Q = torch.Tensor([1, 0, 0, 0])\n",
    "        axis_origin = torch.Tensor([ 0, 0, 0])\n",
    "\n",
    "        # axis angle can be obtained from quaternion\n",
    "        # axis_direction = torch.Tensor([0, 0, 0])\n",
    "\n",
    "        self.Q = nn.Parameter(init_Q, requires_grad = True)\n",
    "        self.axis_origin = nn.Parameter(axis_origin, requires_grad = True)\n",
    "        # self.axis_direction = nn.Parameter(axis_direction, requires_grad = True)\n",
    "\n",
    "\n",
    "    def forward(self, c2w) -> torch.Tensor():\n",
    "        '''\n",
    "        input: c2w\n",
    "        '''\n",
    "        E1 = view2pose_torch(c2w)\n",
    "        translation_matrix = torch.eye(4).to(c2w)\n",
    "        translation_matrix[:3, 3] = self.axis_origin.view([3])\n",
    "        rotation_matrix = torch.eye(4).to(c2w)\n",
    "        R = R_from_quaternions(self.Q)\n",
    "        rotation_matrix[:3, :3] = R\n",
    "        E2 = change_apply_change_basis_torch(E1, rotation_matrix, translation_matrix)\n",
    "        view = pose2view_torch(E2)\n",
    "        return view\n",
    "def fetch_img(root_path, transform_meta, w=640, h=480, device='cuda', if_fix=True):\n",
    "    if if_fix:\n",
    "        idx = 5\n",
    "    else:\n",
    "        idx = np.random.randint(0, 9)\n",
    "    frame_id = 'r_' + str(idx)\n",
    "    pose_np = np.array(transform_meta['frame'][frame_id])\n",
    "\n",
    "    rgb_pil = Image.open(str(root_path/'rgb'/(frame_id + '.png')))\n",
    "    rgb = transforms.ToTensor()(rgb_pil).to(device)\n",
    "    rgb = rgb.view(4, -1).permute(1, 0) # (H*W, 4) RGBA\n",
    "    rgb = rgb[:, :3]*rgb[:, -1:] + (1-rgb[:, -1:]) # blend A to RGB\n",
    "\n",
    "    pose = torch.Tensor(pose_np).to(device)\n",
    "\n",
    "    seg_pil = Image.open(str(root_path/'seg'/(frame_id + '.png')))\n",
    "    seg_np = np.array(seg_pil)\n",
    "    seg = torch.Tensor(seg_np).to(device).view([1, -1]).permute(1, 0)\n",
    "    seg = seg.type(torch.LongTensor)\n",
    "    seg = seg - 1 # starts with 2\n",
    "    seg[seg<0] = 0\n",
    "    focal = transform_meta['focal']\n",
    "    directions = get_ray_directions(h, w, focal).view([-1, 3])\n",
    "    mask = seg > 0\n",
    "    ret_dict = {\n",
    "        'rgb': rgb.to(device),\n",
    "        'seg': seg.to(device),\n",
    "        'directions': directions.to(device),\n",
    "        'pose': pose.to(device),\n",
    "        'mask': mask.to(device)\n",
    "    }\n",
    "    return ret_dict\n",
    "\n",
    "def load_json(json_fname):\n",
    "    with open(json_fname, 'r') as json_file:\n",
    "        data_dict = json.load(json_file)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NeRFSeg models with NeRF checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected keys in pretrained state_dict:\n",
      "coarse_mlp.seg_layer.weight\n",
      "coarse_mlp.seg_layer.bias\n",
      "fine_mlp.seg_layer.weight\n",
      "fine_mlp.seg_layer.bias\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from opt import get_opts\n",
    "\n",
    "sys.argv = ['', '--config', 'config/train_nerfseg.json']\n",
    "device = 'cuda'\n",
    "hparams= get_opts()\n",
    "nerfseg = NeRFSeg(hparams)\n",
    "ckpt = torch.load(hparams.nerf_ckpt)\n",
    "load_dict = {}\n",
    "length = len('model.')\n",
    "state_dict = ckpt['state_dict']\n",
    "for key in state_dict.keys():\n",
    "    if key[:length] == 'model.':\n",
    "        load_dict[key[length:]] = state_dict[key]\n",
    "\n",
    "load_dict_and_report(nerfseg, load_dict)\n",
    "# load_state_dict_and_report(nerfseg, hparams.nerf_ckpt)\n",
    "nerfseg = nerfseg.to(device)\n",
    "art_est = ArticulationEstimation().to(device)\n",
    "near = 2\n",
    "far = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_params = []\n",
    "for name, param in nerfseg.named_parameters():\n",
    "    if 'seg' in name:\n",
    "        seg_params += [param]\n",
    "\n",
    "art_params = []\n",
    "for _, param in art_est.named_parameters():\n",
    "    param.requires_grad = True\n",
    "    art_params += [param]\n",
    "\n",
    "art_lr = 1e-2\n",
    "seg_lr = 1e-3\n",
    "\n",
    "seg_opt_dict = {\n",
    "    'params': seg_params,\n",
    "    'lr': seg_lr\n",
    "}\n",
    "\n",
    "art_opt_dict = {\n",
    "    'params': art_params,\n",
    "    'lr': art_lr\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.Adam([seg_opt_dict, art_opt_dict], lr=seg_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dj/anaconda3/envs/ao/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "optimize_step = 1e3\n",
    "result = []\n",
    "data_root = P(\"./data/laptop_art_same_pose/train/idx_5/\")\n",
    "transform_meta = load_json(str(data_root / 'transforms.json'))\n",
    "ray_chunk_size = 4096\n",
    "data_dict = fetch_img(data_root, transform_meta)\n",
    "\n",
    "directions = data_dict['directions']\n",
    "rgb = data_dict['rgb']\n",
    "seg = data_dict['seg']\n",
    "pose = data_dict['pose']\n",
    "mask = data_dict['mask']\n",
    "random_indx = torch.randint(0, directions.shape[0], [ray_chunk_size//hparams.part_num])\n",
    "random_dirs = directions[random_indx]\n",
    "random_rgbs = rgb[random_indx]\n",
    "random_mask = mask[random_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pose = art_est(pose)\n",
    "rays_o, viewdirs, rays_d = get_rays_torch(random_dirs, new_pose[:3, :], output_view_dirs=True)\n",
    "# gather input_dict for NeRF\n",
    "input_dict = {\n",
    "    'rays_o': rays_o,\n",
    "    'rays_d': rays_d,\n",
    "    'viewdirs': viewdirs\n",
    "}\n",
    "part_code = torch.zeros([1, hparams.part_num]).to(rays_o)\n",
    "part_code[:, int(1)] = 1\n",
    "input_dict['part_code'] = part_code\n",
    "rendered_results = nerfseg(input_dict, False, True, near, far)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_dict = rendered_results['level_0']\n",
    "fine_dict = rendered_results['level_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_dict['comp_seg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function:\n",
    "#   BCE(acc, mask) for non-base part\n",
    "#   porb_i(RGB' - RGB) for all object\n",
    "\n",
    "rgb_c = coarse_dict['rgb']\n",
    "rgb_f = fine_dict['rgb']\n",
    "seg_c = coarse_dict['comp_seg']\n",
    "seg_f = fine_dict['comp_seg']\n",
    "acc_c = coarse_dict['acc']\n",
    "acc_f = coarse_dict['acc']\n",
    "\n",
    "art_dict = {}\n",
    "for i in range(hparams.part_num):\n",
    "    art_dict[str(i)] = ArticulationEstimation() if i != 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for part_id, part_art in art_dict.items():\n",
    "    if part_art is None:\n",
    "        new_pose = pose\n",
    "    else:\n",
    "        new_pose = part_art(pose)\n",
    "    rays_o, viewdirs, rays_d = get_rays_torch(random_dirs, new_pose[:3, :], output_view_dirs=True)\n",
    "    # gather input_dict for NeRF\n",
    "    input_dict = {\n",
    "        'rays_o': rays_o,\n",
    "        'rays_d': rays_d,\n",
    "        'viewdirs': viewdirs\n",
    "    }\n",
    "    part_code = torch.zeros([1, hparams.part_num]).to(rays_o)\n",
    "    part_code[:, int(1)] = 1\n",
    "    input_dict['part_code'] = part_code\n",
    "    rendered_results = nerfseg(input_dict, False, True, near, far)\n",
    "    result_dict[part_id] = rendered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/dj/git/articulated-object-nerf/inerf_seg.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/dj/git/articulated-object-nerf/inerf_seg.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mone_hot(torch\u001b[39m.\u001b[39;49mTensor([\u001b[39m1\u001b[39;49m]), num_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vanilla_nerf.helper import  img2mse_weighted\n",
    "def get_weighted_photo_loss(rendered_results, part_id):\n",
    "    coarse_dict = rendered_results['level_0']\n",
    "    fine_dict = rendered_results['level_1']\n",
    "    rgb_c = coarse_dict['rgb']\n",
    "    rgb_f = fine_dict['rgb']\n",
    "    seg_c = coarse_dict['comp_seg']\n",
    "    seg_f = fine_dict['comp_seg']\n",
    "    # acc_c = coarse_dict['acc']\n",
    "    # acc_f = coarse_dict['acc']\n",
    "    prob_c = seg_c[:, part_id:part_id+1] # [N, 1]\n",
    "    porb_f = seg_f[:, part_id:part_id+1] # [N, 1]\n",
    "    loss_c = img2mse_weighted(x, y, prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_losses = []\n",
    "mask_loses = []\n",
    "for part_id, part_result in result_dict:\n",
    "    if part_id == '0': # base part, only do prob_i(RGB' - RGB)\n",
    "        pass\n",
    "    else:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
